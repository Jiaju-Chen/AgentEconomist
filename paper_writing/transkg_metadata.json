{
  "title": "TransKG: Knowledge Graph Completion with Pre-trained Transformers",
  
  "idea_hypothesis": "Existing knowledge graph completion methods rely on shallow embeddings that struggle to capture complex semantic relationships between entities and relations. We hypothesize that pre-trained Transformer models, with their rich contextual understanding capabilities, can better capture entity semantics and relation patterns, leading to improved link prediction performance, especially in few-shot scenarios where traditional embedding methods suffer from data sparsity.",
  
  "method": "We propose TransKG, a novel knowledge graph completion framework that combines pre-trained BERT models with relation-aware attention mechanisms. The architecture consists of three main components: (1) Entity Encoder: leverages BERT-base with additional entity type embeddings to encode entity descriptions and structural context; (2) Relation-Aware Attention Layer: dynamically adjusts attention weights based on relation types, allowing the model to focus on relation-specific entity features; (3) Contrastive Scoring Module: uses contrastive learning with negative sampling to learn discriminative entity-relation representations. The model is trained end-to-end with a margin-based ranking loss combined with a contrastive objective.",

  "data": "We evaluate TransKG on three standard knowledge graph benchmark datasets: FB15k-237 (14,541 entities, 237 relations, 310,116 triples), WN18RR (40,943 entities, 11 relations, 93,003 triples), and YAGO3-10 (123,182 entities, 37 relations, 1,089,040 triples). We use the standard train/validation/test splits provided by previous work. For few-shot experiments, we create subsets with 10%, 30%, and 50% of training triples.",
  
  "experiments": "We compare TransKG against several baseline methods including TransE, RotatE, ComplEx, TuckER, and recent Transformer-based methods like KG-BERT and StAR. Evaluation metrics include Mean Reciprocal Rank (MRR) and Hits@1/3/10. Main results show that TransKG achieves state-of-the-art performance on FB15k-237 with MRR of 0.391, outperforming the strongest baseline RotatE by 3.2%. On WN18RR, we achieve 0.478 MRR (+2.1% over baseline). Key findings from ablation studies: (1) Relation-aware attention contributes +2.1% MRR improvement; (2) Contrastive learning adds +1.5% on average; (3) In few-shot settings (10% data), TransKG shows +5.8% improvement over RotatE, demonstrating better generalization.",
  
  "references": [
    "@inproceedings{bordes2013transE,\n  title={Translating embeddings for modeling multi-relational data},\n  author={Bordes, Antoine and Usunier, Nicolas and Garcia-Duran, Alberto and Weston, Jason and Yakhnenko, Oksana},\n  booktitle={Advances in Neural Information Processing Systems},\n  pages={2787--2795},\n  year={2013}\n}",
    "@inproceedings{sun2019rotate,\n  title={RotatE: Knowledge graph embedding by relational rotation in complex space},\n  author={Sun, Zhiqing and Deng, Zhi-Hong and Nie, Jian-Yun and Tang, Jian},\n  booktitle={International Conference on Learning Representations},\n  year={2019}\n}",
    "@inproceedings{yao2019kgbert,\n  title={KG-BERT: BERT for knowledge graph completion},\n  author={Yao, Liang and Mao, Chengsheng and Luo, Yuan},\n  booktitle={arXiv preprint arXiv:1909.03193},\n  year={2019}\n}",
    "@inproceedings{devlin2019bert,\n  title={BERT: Pre-training of deep bidirectional transformers for language understanding},\n  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},\n  booktitle={Proceedings of NAACL-HLT},\n  pages={4171--4186},\n  year={2019}\n}",
    "@inproceedings{wang2019knowledge,\n  title={Knowledge graph embedding: A survey of approaches and applications},\n  author={Wang, Quan and Mao, Zhendong and Wang, Bin and Guo, Li},\n  booktitle={IEEE Transactions on Knowledge and Data Engineering},\n  volume={29},\n  number={12},\n  pages={2724--2743},\n  year={2017}\n}"
  ],
  
  "figures": [
    {
      "id": "fig:architecture",
      "file_path": "example_jsons/figures/architecture.pdf",
      "caption": "Overview of the TransKG architecture",
      "description": "Shows the three main components: Entity Encoder with BERT, Relation-Aware Attention Layer, and Contrastive Scoring Module"
    },
    {
      "id": "fig:ablation",
      "file_path": "example_jsons/figures/results_bar.pdf",
      "caption": "Ablation study results on FB15k-237",
      "description": "Bar chart comparing MRR scores with and without each component across different data ratios"
    }
  ],
  
  "tables": [
    {
      "id": "tab:main_results",
      "caption": "Link prediction results on FB15k-237 and WN18RR",
      "content": "Model | FB15k-237 MRR | FB15k-237 Hits@10 | WN18RR MRR | WN18RR Hits@10\nTransE | 0.294 | 0.465 | 0.226 | 0.501\nRotatE | 0.338 | 0.533 | 0.476 | 0.571\nComplEx | 0.325 | 0.521 | 0.440 | 0.510\nKG-BERT | 0.351 | 0.544 | 0.461 | 0.548\nTransKG (ours) | 0.391 | 0.561 | 0.478 | 0.582",
      "description": "Comparison of TransKG with baseline methods. Bold indicates best results."
    },
    {
      "id": "tab:ablation",
      "caption": "Ablation study on component contributions",
      "content": "Configuration | MRR | Hits@10\nFull Model | 0.391 | 0.561\nw/o Relation-Aware Attention | 0.370 | 0.532\nw/o Contrastive Learning | 0.376 | 0.544\nw/o Both | 0.351 | 0.518",
      "description": "Effect of removing each component from the full model"
    }
  ],
  
  "template_path": "example_jsons/icml2026.zip",
  "style_guide": "ICML",
  "target_pages": 8,
  
  "enable_vlm_review": true
}
